{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, n_hidden, n_class, n_layers=1, dropout=0.5):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_class = n_class\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # 编码器\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=n_class,\n",
    "            hidden_size=n_hidden,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0  # 只有多层时才使用dropout\n",
    "        )\n",
    "        \n",
    "        # 解码器\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=n_class,\n",
    "            hidden_size=n_hidden,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # 线性层\n",
    "        self.linear = nn.Linear(n_hidden, n_class)\n",
    "    \n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        # 编码器\n",
    "        enc_input = enc_input.permute(1, 0, 2)  # 调整维度为 (seq_len, batch_size, input_size)\n",
    "        enc_output, enc_hidden = self.encoder(enc_input, enc_hidden)\n",
    "        \n",
    "        # 解码器\n",
    "        dec_input = dec_input.permute(1, 0, 2)  # 调整维度为 (seq_len, batch_size, input_size)\n",
    "        dec_output, dec_hidden = self.decoder(dec_input, enc_hidden)\n",
    "        \n",
    "        # 线性层\n",
    "        output = self.linear(dec_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m output = model(input_batch, hidden, output_batch)\n\u001b[32m     77\u001b[39m output = output.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m loss = criterion(\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_class\u001b[49m\u001b[43m)\u001b[49m, target_batch.view(-\u001b[32m1\u001b[39m))\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (epoch + \u001b[32m1\u001b[39m) % \u001b[32m1000\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEpoch:\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[38;5;132;01m%04d\u001b[39;00m\u001b[33m'\u001b[39m % (epoch + \u001b[32m1\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mcost =\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[33m'\u001b[39m.format(loss.item()))\n",
      "\u001b[31mRuntimeError\u001b[39m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, n_hidden, n_class):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.decoder = nn.LSTM(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.linear = nn.Linear(n_hidden, n_class)\n",
    "        \n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.permute(1, 0, 2)\n",
    "        dec_input = dec_input.permute(1, 0, 2)\n",
    "        _, enc_hidden = self.encoder(enc_input, enc_hidden)\n",
    "        output, _ = self.decoder(dec_input, enc_hidden)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "def make_batch(seq_data, num_dic, n_step, n_class):\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + \"P\" * (n_step - len(seq[i]))\n",
    "\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        output = [num_dic[n] for n in (\"S\" + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + \"E\")]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        output_batch.append(np.eye(n_class)[output])\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return (\n",
    "        torch.FloatTensor(input_batch),\n",
    "        torch.FloatTensor(output_batch),\n",
    "        torch.LongTensor(target_batch),\n",
    "    )\n",
    "    \n",
    "def make_testbatch(input_word, num_dic, n_step, n_class):\n",
    "    input_batch, output_batch = [], []\n",
    "\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))\n",
    "    input = [num_dic[n] for n in input_w]\n",
    "    output = [num_dic[n] for n in 'S' + 'P' * n_step]\n",
    "\n",
    "    input_batch = np.eye(n_class)[input]\n",
    "    output_batch = np.eye(n_class)[output]\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_step = 5\n",
    "    n_hidden = 128\n",
    "\n",
    "    char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "    num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "    seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "    n_class = len(num_dic)\n",
    "    batch_size = len(seq_data)\n",
    "\n",
    "    model = Seq2Seq(n_hidden, n_class)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch(seq_data, num_dic, n_step, n_class)\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        hidden = (torch.zeros(1, batch_size, n_hidden), torch.zeros(1, batch_size, n_hidden))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        output = output.transpose(0, 1)\n",
    "        loss = criterion(output.view(-1, n_class), target_batch.view(-1))\n",
    "        \n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    def translate(word):\n",
    "        input_batch, output_batch = make_testbatch(word, num_dic, n_step, n_class)\n",
    "\n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        hidden = (torch.zeros(1, 1, n_hidden), torch.zeros(1, 1, n_hidden))\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        predict = output.data.max(2, keepdim=True)[1]\n",
    "        decoded = [char_arr[i] for i in predict]\n",
    "        end = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "\n",
    "        return translated.replace('P', '')\n",
    "\n",
    "    print('test')\n",
    "    print('man ->', translate('man'))\n",
    "    print('mans ->', translate('mans'))\n",
    "    print('king ->', translate('king'))\n",
    "    print('black ->', translate('black'))\n",
    "    print('upp ->', translate('upp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.003528\n",
      "Epoch: 2000 cost = 0.000962\n",
      "Epoch: 3000 cost = 0.000410\n",
      "Epoch: 4000 cost = 0.000206\n",
      "Epoch: 5000 cost = 0.000111\n",
      "test\n",
      "man -> women\n",
      "mans -> women\n",
      "king -> queen\n",
      "black -> white\n",
      "upp -> down\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# code by Tae Hwan Jung @graykode\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        output_batch.append(np.eye(n_class)[output])\n",
    "        target_batch.append(target) # not one-hot\n",
    "\n",
    "    # make tensor\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# make test batch\n",
    "def make_testbatch(input_word):\n",
    "    input_batch, output_batch = [], []\n",
    "\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))\n",
    "    input = [num_dic[n] for n in input_w]\n",
    "    output = [num_dic[n] for n in 'S' + 'P' * n_step]\n",
    "\n",
    "    input_batch = np.eye(n_class)[input]\n",
    "    output_batch = np.eye(n_class)[output]\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0, 1) # enc_input: [max_len(=n_step, time step), batch_size, n_class]\n",
    "        dec_input = dec_input.transpose(0, 1) # dec_input: [max_len(=n_step, time step), batch_size, n_class]\n",
    "\n",
    "        # enc_states : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "        # outputs : [max_len+1(=6), batch_size, num_directions(=1) * n_hidden(=128)]\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        model = self.fc(outputs) # model : [max_len+1(=6), batch_size, n_class]\n",
    "        return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_step = 5\n",
    "    n_hidden = 128\n",
    "\n",
    "    char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "    num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "    seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "    n_class = len(num_dic)\n",
    "    batch_size = len(seq_data)\n",
    "\n",
    "    model = Seq2Seq()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
    "        hidden = torch.zeros(1, batch_size, n_hidden)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # input_batch : [batch_size, max_len(=n_step, time step), n_class]\n",
    "        # output_batch : [batch_size, max_len+1(=n_step, time step) (becase of 'S' or 'E'), n_class]\n",
    "        # target_batch : [batch_size, max_len+1(=n_step, time step)], not one-hot\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len+1, batch_size, n_class]\n",
    "        output = output.transpose(0, 1) # [batch_size, max_len+1(=6), n_class]\n",
    "        loss = 0\n",
    "        for i in range(0, len(target_batch)):\n",
    "            # output[i] : [max_len+1, n_class, target_batch[i] : max_len+1]\n",
    "            loss += criterion(output[i], target_batch[i])\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    def translate(word):\n",
    "        input_batch, output_batch = make_testbatch(word)\n",
    "\n",
    "        # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
    "        hidden = torch.zeros(1, 1, n_hidden)\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len+1(=6), batch_size(=1), n_class]\n",
    "\n",
    "        predict = output.data.max(2, keepdim=True)[1] # select n_class dimension\n",
    "        decoded = [char_arr[i] for i in predict]\n",
    "        end = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "\n",
    "        return translated.replace('P', '')\n",
    "\n",
    "    print('test')\n",
    "    print('man ->', translate('man'))\n",
    "    print('mans ->', translate('mans'))\n",
    "    print('king ->', translate('king'))\n",
    "    print('black ->', translate('black'))\n",
    "    print('upp ->', translate('upp'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
